import json
import uuid
from datetime import datetime
from azure.search.documents import SearchClient
from azure_config import azure_config


class DocumentProcessor:
    def __init__(self):
        self.search_client = azure_config.get_search_client()
        self.openai_client = azure_config.get_openai_client()
        self.deployment_name = azure_config.openai_deployment_name

    def chunk_text(self, text, chunk_size=1500, overlap=150):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size
            if end > len(text):
                end = len(text)

            chunk = text[start:end]
            chunks.append(chunk)

            if end == len(text):
                break

            start = end - overlap

        return chunks

    def index_document(self, document_result):
        """ë¬¸ì„œë¥¼ AI Searchì— ì¸ë±ì‹± - onboarding-index ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ìˆ˜ì •"""
        try:
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = self.chunk_text(document_result["extracted_text"])

            # ê° ì²­í¬ë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì¸ë±ì‹±
            documents = []
            for i, chunk in enumerate(chunks):
                # ê°„ë‹¨í•œ Key ìƒì„± (ë¬¸ì, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´, ëŒ€ì‹œë§Œ ì‚¬ìš©)
                clean_doc_id = document_result["document_id"].replace("-", "")
                storage_path = f"doc_{clean_doc_id}_chunk_{i}"

                document = {
                    # í•„ìˆ˜ key í•„ë“œ
                    "metadata_storage_path": storage_path,
                    # ì‹¤ì œ ìŠ¤í‚¤ë§ˆì— ìˆëŠ” í•„ë“œë“¤ë§Œ ì‚¬ìš©
                    # ì‹¤ì œ onboarding-index ìŠ¤í‚¤ë§ˆì— ìˆëŠ” í•„ë“œë“¤ë§Œ ì‚¬ìš©
                    "content": chunk,
                    "merged_content": chunk,
                    "text": [chunk],  # Collection íƒ€ì…
                    "layoutText": [chunk],  # Collection íƒ€ì…
                    # ë©”íƒ€ë°ì´í„° í•„ë“œë“¤ (ìŠ¤í‚¤ë§ˆì— ìˆëŠ” ê²ƒë“¤ë§Œ)
                    "metadata_storage_size": len(chunk.encode("utf-8")),
                    "metadata_storage_last_modified": datetime.now().isoformat() + "Z",
                    "metadata_storage_content_type": "text/plain",
                    "metadata_storage_file_extension": document_result["file_type"],
                    "metadata_storage_name": document_result["file_name"]
                    # ë¹ˆ ì»¬ë ‰ì…˜ë“¤ (ìŠ¤í‚¤ë§ˆì— ìˆëŠ” ê²ƒë“¤)
                    "people": [],
                    "organizations": [],
                    "locations": [],  # ìŠ¤í‚¤ë§ˆì— ìˆìŒ
                    "keyphrases": [],
                    "pii_entities": [],
                    "imageTags": [],
                    "imageCaption": []
                }

                documents.append(document)

            # AI Searchì— ë¬¸ì„œë“¤ ì—…ë¡œë“œ
            result = self.search_client.upload_documents(documents)

            return {
                "success": True,
                "indexed_chunks": len(documents),
                "document_id": document_result["document_id"],
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def generate_document_summary(self, document_result):
        """ë¬¸ì„œ ìš”ì•½ ìƒì„±"""
        try:
            # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ ì²˜ë¦¬
            text = document_result["extracted_text"]

            # ìš”ì•½ í”„ë¡¬í”„íŠ¸
            summary_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

ë¬¸ì„œëª…: {document_result["file_name"]}
ë¬¸ì„œ íƒ€ì…: {document_result["file_type"]}

ë‚´ìš©:
{text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ë¬¸ì„œ ê°œìš” (2-3ì¤„)
2. ì£¼ìš” ë‚´ìš© (3-5ê°œ ìš”ì )
3. í•µì‹¬ ê¸°ìˆ /ì‹œìŠ¤í…œ (ìˆëŠ” ê²½ìš°)
4. ì¤‘ìš” ì°¸ê³ ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.",
                    },
                    {"role": "user", "content": summary_prompt},
                ],
                max_completion_tokens=7000,
            )

            summary = response.choices[0].message.content

            return {
                "success": True,
                "summary": summary,
                "document_id": document_result["document_id"],
                "file_name": document_result["file_name"],
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def extract_technical_info(self, document_result):
        """ê¸°ìˆ  ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            text = document_result["extracted_text"]
            if len(text) > 4000:
                text = text[:4000] + "..."

            tech_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ ê¸°ìˆ  í‚¤ì›Œë“œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{text}

ê¸°ìˆ  í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•´ì„œ ì½¤ë§ˆë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš” (ì˜ˆ: Python, React, Docker, AWS):"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "ê¸°ìˆ  ë¬¸ì„œì—ì„œ ê¸°ìˆ  í‚¤ì›Œë“œë§Œ ê°„ëµí•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": tech_prompt},
                ],
                max_completion_tokens=7000,
            )

            tech_keywords = response.choices[0].message.content

            return {
                "success": True,
                "technical_keywords": tech_keywords,
                "document_id": document_result["document_id"],
                "file_name": document_result["file_name"],
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def generate_integrated_tech_guide(self, processed_files):
        """ì „ì²´ ë¬¸ì„œë“¤ì„ í†µí•©í•´ì„œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œ ìƒì„±"""
        try:
            # ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
            all_content = []
            tech_keywords = []

            for file_result in processed_files:
                if file_result.get("success") and "extracted_text" in file_result:
                    # ë¬¸ì„œ ë‚´ìš© ì¼ë¶€ ì¶”ê°€
                    content = file_result["extracted_text"][
                        :2000
                    ]  # ê° ë¬¸ì„œë‹¹ 2000ìê¹Œì§€
                    all_content.append(f"[{file_result['file_name']}]\n{content}")

                    # ê¸°ìˆ  í‚¤ì›Œë“œ ìˆ˜ì§‘
                    if "processing_results" in file_result:
                        tech_result = file_result["processing_results"].get(
                            "technical_info", {}
                        )
                        if tech_result.get("success"):
                            tech_keywords.append(
                                tech_result.get("technical_keywords", "")
                            )

            # í†µí•© ì»¨í…ì¸  ìƒì„±
            combined_content = "\n\n".join(all_content)
            combined_keywords = ", ".join([k for k in tech_keywords if k])

            # í†µí•© ê¸°ìˆ  ê°€ì´ë“œ í”„ë¡¬í”„íŠ¸
            guide_prompt = f"""ë‹¤ìŒì€ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê¸°ì¡´ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ê·œ íˆ¬ì…ìë¥¼ ìœ„í•œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{combined_content[:8000]}  # ìµœëŒ€ 8000ìê¹Œì§€

ë°œê²¬ëœ ê¸°ìˆ  í‚¤ì›Œë“œë“¤:
{combined_keywords}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•™ìŠµ ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

### ğŸš€ í”„ë¡œì íŠ¸ ê¸°ìˆ  ìŠ¤íƒ
- ì£¼ìš” ê¸°ìˆ ë“¤ì˜ ê°„ë‹¨í•œ ì„¤ëª…

### ğŸ“š ìš°ì„  í•™ìŠµ ê¸°ìˆ  (ì¤‘ìš”ë„ ìˆœ)
1. **ê¸°ìˆ ëª…1**: í•™ìŠµ ì´ìœ  ë° ì¤‘ìš”ë„
2. **ê¸°ìˆ ëª…2**: í•™ìŠµ ì´ìœ  ë° ì¤‘ìš”ë„
...

## ğŸ“– ì¶”ì²œ í•™ìŠµ ë¦¬ì†ŒìŠ¤
- ê° ê¸°ìˆ ë³„ ì¶”ì²œ ë¬¸ì„œë‚˜ íŠœí† ë¦¬ì–¼"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ ê°œë°œìë¥¼ ìœ„í•œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ í†µí•©í•˜ì—¬ ì‹ ê·œ íˆ¬ì…ìë¥¼ ìœ„í•œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": guide_prompt},
                ],
                max_completion_tokens=7000,
            )

            tech_guide = response.choices[0].message.content

            return {
                "success": True,
                "tech_guide": tech_guide,
                "processed_files_count": len(processed_files),
                "total_keywords": combined_keywords,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def search_documents(self, query, top_k=5):
        """ë¬¸ì„œ ê²€ìƒ‰ - ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ìˆ˜ì •ëœ ë²„ì „"""
        try:
            # retrievable=trueì¸ í•„ë“œë“¤ë§Œ selectì— ì‚¬ìš©
            search_results = self.search_client.search(
                search_text=query,
                top=top_k,
                include_total_count=True,
                select=[
                    "content",
                    "merged_content",
                    "metadata_storage_path",
                    "metadata_storage_name",  # ì‹¤ì œ íŒŒì¼ëª… í•„ë“œ ì¶”ê°€
                ],
                query_type="simple",
                search_mode="all",
            )

            results = []
            for result in search_results:
                # ì•ˆì „í•˜ê²Œ í•„ë“œ ì ‘ê·¼
                content = result.get("content") or result.get("merged_content", "")

                # ì‹¤ì œ íŒŒì¼ëª… ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                file_name = result.get("metadata_storage_name")
                if not file_name:
                    # ê¸°ì¡´ ë°©ì‹: metadata_storage_pathì—ì„œ ì¶”ì¶œ
                    storage_path = result.get("metadata_storage_path", "")
                    file_name = "ì—…ë¡œë“œëœ ë¬¸ì„œ"  # ê¸°ë³¸ê°’

                    if (
                        storage_path
                        and "doc_" in storage_path
                        and "_chunk_" in storage_path
                    ):
                        try:
                            # doc_UUID_chunk_N í˜•ì‹ì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                            parts = storage_path.split("_")
                            if len(parts) >= 3:
                                uuid_part = parts[1][:8]  # UUID ì• 8ìë¦¬
                                file_name = f"ë¬¸ì„œ_{uuid_part}"
                        except:
                            file_name = "ì—…ë¡œë“œëœ ë¬¸ì„œ"
                results.append(
                    {
                        "content": content,
                        "file_name": file_name,
                        "score": result["@search.score"],
                        "storage_path": result.get("metadata_storage_path", "")
                    }
                )

            # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë¶€ë¶„ ê²€ìƒ‰ë„ ì‹œë„
            if len(results) < 2:
                fallback_results = self.search_client.search(
                    search_text=query,
                    top=top_k,
                    include_total_count=True,
                    select=[
                        "content",
                        "merged_content",
                        "metadata_storage_path",
                        "metadata_storage_name",
                    ],
                    query_type="simple",
                    search_mode="any",
                )

                existing_paths = {r["storage_path"] for r in results}
                for result in fallback_results:
                    current_path = result.get("metadata_storage_path", "")
                    if current_path not in existing_paths:
                        content = result.get("content") or result.get(
                            "merged_content", ""
                        )

                        # ì‹¤ì œ íŒŒì¼ëª… ìš°ì„  ì‚¬ìš©
                        file_name = result.get("metadata_storage_name")
                        if not file_name:
                            file_name = "ì—…ë¡œë“œëœ ë¬¸ì„œ"
                            if (
                                current_path
                                and "doc_" in current_path
                                and "_chunk_" in current_path
                            ):
                                try:
                                    parts = current_path.split("_")
                                    if len(parts) >= 3:
                                        uuid_part = parts[1][:8]
                                        file_name = f"ë¬¸ì„œ_{uuid_part}"
                                except:
                                    pass

                        results.append(
                            {
                                "content": content,
                                "file_name": file_name,
                                "score": result["@search.score"],
                                "storage_path": current_path,
                            }
                        )

                        if len(results) >= top_k:
                            break

            return {"success": True, "results": results, "total_count": len(results)}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def answer_question(self, question, search_results=None):
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (RAG + ì¼ë°˜ ì§€ì‹)"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ ìˆ˜í–‰
            if search_results is None:
                search_result = self.search_documents(question)
                if not search_result["success"]:
                    raise Exception(f"ê²€ìƒ‰ ì‹¤íŒ¨: {search_result['error']}")
                search_results = search_result["results"]

            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            context = ""
            sources = []
            if search_results:
                context_parts = []
                for result in search_results[:3]:  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
                    context_parts.append(
                        f"[ë¬¸ì„œ: {result['file_name']}]\n{result['content']}"
                    )
                    sources.append(result["file_name"])
                context = "\n\n".join(context_parts)

            # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ - ë¬¸ì„œ ê¸°ë°˜ + ì¼ë°˜ ì§€ì‹
            if context.strip():
                # ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
                prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œ:
{context}

ë‹µë³€ ê·œì¹™:
1. ë¨¼ì € ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë˜, ì´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”
4. ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ê³ í•œ ë¬¸ì„œë¥¼ ëª…ì‹œí•˜ì„¸ìš”

ë‹µë³€ í˜•ì‹:
### ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
### ì¶”ê°€ ì¼ë°˜ ì§€ì‹ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)

**ì°¸ê³  ë¬¸ì„œ:** [ë¬¸ì„œëª…ë“¤]"""

                answer_type = "document_based"
            else:
                # ì¼ë°˜ ì§€ì‹ ê¸°ë°˜ ë‹µë³€
                prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. í”„ë¡œì íŠ¸ ì‹ ê·œ íˆ¬ì… ë° ê¸°ìˆ  í•™ìŠµ ê´€ì ì—ì„œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
2. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”
3. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë°©ë²•ì„ í¬í•¨í•˜ì„¸ìš”
4. ë‹µë³€ ë§ˆì§€ë§‰ì— "â€» ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í–ˆìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”"""

                answer_type = "general_knowledge"

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ìˆ˜í–‰ ì¤‘ ì‹ ê·œ íˆ¬ì…ìì—ê²Œ ì¸ìˆ˜ì¸ê³„ë¥¼ í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ìˆ  ë¬¸ì„œì™€ ì¼ë°˜ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì‹ ê·œ íˆ¬ì…ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_completion_tokens=1500,
            )

            answer = response.choices[0].message.content

            return {
                "success": True,
                "answer": answer,
                "answer_type": answer_type,
                "sources": sources,
                "search_results": search_results,
                "search_result_count": len(search_results) if search_results else 0,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def process_document_complete(self, document_result):
        """ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        results = {
            "document_id": document_result["document_id"],
            "file_name": document_result["file_name"],
            "processing_results": {},
        }

        # 1. AI Search ì¸ë±ì‹±
        print("AI Search ì¸ë±ì‹± ì¤‘...")
        index_result = self.index_document(document_result)
        results["processing_results"]["indexing"] = index_result

        # 2. ë¬¸ì„œ ìš”ì•½ ìƒì„±
        print("ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘...")
        summary_result = self.generate_document_summary(document_result)
        results["processing_results"]["summary"] = summary_result

        # 3. ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í™”)
        print("ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        tech_result = self.extract_technical_info(document_result)
        results["processing_results"]["technical_info"] = tech_result

        return results


# ì „ì—­ í”„ë¡œì„¸ì„œ ê°ì²´
document_processor = DocumentProcessor()

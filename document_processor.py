# document_processor.py (ê°„ë‹¨í•œ Key ì‚¬ìš©)
import json
import uuid
from datetime import datetime
from azure.search.documents import SearchClient
from azure_config import azure_config

class DocumentProcessor:
    def __init__(self):
        self.search_client = azure_config.get_search_client()
        self.openai_client = azure_config.get_openai_client()
        self.deployment_name = azure_config.openai_deployment_name
    
    def chunk_text(self, text, chunk_size=1500, overlap=150):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            if end > len(text):
                end = len(text)
            
            chunk = text[start:end]
            chunks.append(chunk)
            
            if end == len(text):
                break
                
            start = end - overlap
        
        return chunks
    
    def index_document(self, document_result):
        """ë¬¸ì„œë¥¼ AI Searchì— ì¸ë±ì‹± - ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í™œìš© (ê°„ë‹¨í•œ Key)"""
        try:
            # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = self.chunk_text(document_result["extracted_text"])
            
            # ê° ì²­í¬ë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì¸ë±ì‹±
            documents = []
            for i, chunk in enumerate(chunks):
                # ê°„ë‹¨í•œ Key ìƒì„± (ë¬¸ì, ìˆ«ì, ì–¸ë”ìŠ¤ì½”ì–´, ëŒ€ì‹œë§Œ ì‚¬ìš©)
                # UUIDì—ì„œ í•˜ì´í”ˆ ì œê±°í•˜ê³  ì²­í¬ ì¸ë±ìŠ¤ ì¶”ê°€
                clean_doc_id = document_result['document_id'].replace('-', '')
                storage_path = f"doc_{clean_doc_id}_chunk_{i}"
                
                document = {
                    # ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì˜ í•„ìˆ˜ í•„ë“œë“¤
                    "metadata_storage_path": storage_path,  # key í•„ë“œ (ê°„ë‹¨í•œ í˜•ì‹)
                    "metadata_storage_name": f"{document_result['file_name']}_chunk_{i}",
                    "metadata_storage_size": len(chunk.encode('utf-8')),
                    "metadata_storage_last_modified": datetime.now().isoformat() + "Z",
                    "metadata_storage_content_type": "text/plain",
                    "metadata_storage_file_extension": document_result["file_type"],
                    
                    # ì»¨í…ì¸  í•„ë“œë“¤
                    "content": chunk,
                    "merged_content": chunk,
                    "text": [chunk],  # Collection íƒ€ì…
                    "layoutText": [chunk],  # Collection íƒ€ì…
                    
                    # ì–¸ì–´ ê´€ë ¨
                    "language": "ko",
                    "metadata_language": "ko",
                    
                    # ë‚ ì§œ í•„ë“œ
                    "metadata_creation_date": datetime.now().isoformat() + "Z",
                    
                    # ë¹ˆ ì»¬ë ‰ì…˜ë“¤ (ì˜¤ë¥˜ ë°©ì§€)
                    "organizations": [],
                    "keyphrases": [],
                    "pii_entities": []
                }
                
                documents.append(document)
            
            # AI Searchì— ë¬¸ì„œë“¤ ì—…ë¡œë“œ
            result = self.search_client.upload_documents(documents)
            
            return {
                "success": True,
                "indexed_chunks": len(documents),
                "document_id": document_result["document_id"]
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def generate_document_summary(self, document_result):
        """ë¬¸ì„œ ìš”ì•½ ìƒì„±"""
        try:
            # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ ì²˜ë¦¬
            text = document_result["extracted_text"]
            # if len(text) > 8000:
            #     text = text[:8000] + "..."
            
            # ìš”ì•½ í”„ë¡¬í”„íŠ¸
            summary_prompt = f"""ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:

ë¬¸ì„œëª…: {document_result["file_name"]}
ë¬¸ì„œ íƒ€ì…: {document_result["file_type"]}

ë‚´ìš©:
{text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. ë¬¸ì„œ ê°œìš” (2-3ì¤„)
2. ì£¼ìš” ë‚´ìš© (3-5ê°œ ìš”ì )
3. í•µì‹¬ ê¸°ìˆ /ì‹œìŠ¤í…œ (ìˆëŠ” ê²½ìš°)
4. ì¤‘ìš” ì°¸ê³ ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í”„ë¡œì íŠ¸ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": summary_prompt}
                ],
                max_completion_tokens=7000
            )
            
            summary = response.choices[0].message.content
            
            return {
                "success": True,
                "summary": summary,
                "document_id": document_result["document_id"],
                "file_name": document_result["file_name"]
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def extract_technical_info(self, document_result):
        """ê¸°ìˆ  ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            text = document_result["extracted_text"]
            if len(text) > 4000:
                text = text[:4000] + "..."
            
            tech_prompt = f"""ë‹¤ìŒ ë¬¸ì„œì—ì„œ ê¸°ìˆ  í‚¤ì›Œë“œë“¤ì„ ê°„ë‹¨íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{text}

ê¸°ìˆ  í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•´ì„œ ì½¤ë§ˆë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš” (ì˜ˆ: Python, React, Docker, AWS):"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ê¸°ìˆ  ë¬¸ì„œì—ì„œ ê¸°ìˆ  í‚¤ì›Œë“œë§Œ ê°„ë‹¨íˆ ì¶”ì¶œí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": tech_prompt}
                ],
                max_completion_tokens=7000
            )
            
            tech_keywords = response.choices[0].message.content
            
            return {
                "success": True,
                "technical_keywords": tech_keywords,
                "document_id": document_result["document_id"],
                "file_name": document_result["file_name"]
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def generate_integrated_tech_guide(self, processed_files):
        """ì „ì²´ ë¬¸ì„œë“¤ì„ í†µí•©í•´ì„œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œ ìƒì„±"""
        try:
            # ëª¨ë“  ë¬¸ì„œì˜ ë‚´ìš©ì„ ìˆ˜ì§‘
            all_content = []
            tech_keywords = []
            
            for file_result in processed_files:
                if file_result.get("success") and "extracted_text" in file_result:
                    # ë¬¸ì„œ ë‚´ìš© ì¼ë¶€ ì¶”ê°€
                    content = file_result["extracted_text"][:2000]  # ê° ë¬¸ì„œë‹¹ 2000ìê¹Œì§€
                    all_content.append(f"[{file_result['file_name']}]\n{content}")
                    
                    # ê¸°ìˆ  í‚¤ì›Œë“œ ìˆ˜ì§‘
                    if "processing_results" in file_result:
                        tech_result = file_result["processing_results"].get("technical_info", {})
                        if tech_result.get("success"):
                            tech_keywords.append(tech_result.get("technical_keywords", ""))
            
            # í†µí•© ì»¨í…ì¸  ìƒì„±
            combined_content = "\n\n".join(all_content)
            combined_keywords = ", ".join([k for k in tech_keywords if k])
            
            # í†µí•© ê¸°ìˆ  ê°€ì´ë“œ í”„ë¡¬í”„íŠ¸
            guide_prompt = f"""ë‹¤ìŒì€ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ë¬¸ì„œë“¤ì…ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ê·œ íˆ¬ì…ìë¥¼ ìœ„í•œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{combined_content[:8000]}  # ìµœëŒ€ 8000ìê¹Œì§€

ë°œê²¬ëœ ê¸°ìˆ  í‚¤ì›Œë“œë“¤:
{combined_keywords}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í•™ìŠµ ê°€ì´ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

## ğŸš€ í”„ë¡œì íŠ¸ ê¸°ìˆ  ìŠ¤íƒ
- ì£¼ìš” ê¸°ìˆ ë“¤ì˜ ê°„ë‹¨í•œ ì„¤ëª…

## ğŸ“š ìš°ì„  í•™ìŠµ ê¸°ìˆ  (ì¤‘ìš”ë„ ìˆœ)
1. **ê¸°ìˆ ëª…1**: í•™ìŠµ ì´ìœ  ë° ì¤‘ìš”ë„
2. **ê¸°ìˆ ëª…2**: í•™ìŠµ ì´ìœ  ë° ì¤‘ìš”ë„
...

## ğŸ“– ì¶”ì²œ í•™ìŠµ ë¦¬ì†ŒìŠ¤
- ê° ê¸°ìˆ ë³„ ì¶”ì²œ ë¬¸ì„œë‚˜ íŠœí† ë¦¬ì–¼"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê°œë°œìë¥¼ ìœ„í•œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹ ê·œ íˆ¬ì…ìë¥¼ ìœ„í•œ ê¸°ìˆ  í•™ìŠµ ê°€ì´ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."},
                    {"role": "user", "content": guide_prompt}
                ],
                max_completion_tokens=7000
            )
            
            tech_guide = response.choices[0].message.content
            
            return {
                "success": True,
                "tech_guide": tech_guide,
                "processed_files_count": len(processed_files),
                "total_keywords": combined_keywords
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
        

    def answer_question(self, question, search_results=None):
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (RAG)"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê²€ìƒ‰ ìˆ˜í–‰
            if search_results is None:
                search_result = self.search_documents(question)
                if not search_result["success"]:
                    raise Exception(f"ê²€ìƒ‰ ì‹¤íŒ¨: {search_result['error']}")
                search_results = search_result["results"]
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
            context = ""
            if search_results:
                context = "\n\n".join([
                    f"[ë¬¸ì„œ: {result['file_name']}]\n{result['content']}"
                    for result in search_results[:3]  # ìƒìœ„ 3ê°œë§Œ ì‚¬ìš©
                ])
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if not context.strip():
                return {
                    "success": True,
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì‹œê±°ë‚˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
                    "sources": [],
                    "search_results": []
                }
            
            # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì°¸ê³  ë¬¸ì„œ:
{context}

ë‹µë³€ ê·œì¹™:
1. ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë³´ì™„í•˜ë˜, ì´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
3. í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”
4. ë¬¸ì„œ ì¶œì²˜ë¥¼ ë‹µë³€ì— í¬í•¨í•˜ì„¸ìš”"""

            response = self.openai_client.chat.completions.create(
                model=self.deployment_name,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_completion_tokens=1000
            )
            
            answer = response.choices[0].message.content
            
            return {
                "success": True,
                "answer": answer,
                "sources": [result["file_name"] for result in search_results] if search_results else [],
                "search_results": search_results
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }       
    
    def search_documents(self, query, top_k=3):
        """ë¬¸ì„œ ê²€ìƒ‰ - ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ í•„ë“œ ì‚¬ìš©"""
        try:
            search_results = self.search_client.search(
                search_text=query,
                top=top_k,
                include_total_count=True,
                select=["content", "merged_content", "metadata_storage_name", "metadata_storage_path"]
            )
            
            results = []
            for result in search_results:
                # ì•ˆì „í•˜ê²Œ í•„ë“œ ì ‘ê·¼
                content = result.get("content") or result.get("merged_content", "")
                file_name = result.get("metadata_storage_name", "Unknown")
                
                # ì›ë³¸ íŒŒì¼ëª… ì¶”ì¶œ (chunk ì •ë³´ ì œê±°)
                if "_chunk_" in file_name:
                    file_name = file_name.split("_chunk_")[0]
                
                results.append({
                    "content": content,
                    "file_name": file_name,
                    "score": result["@search.score"],
                    "storage_path": result.get("metadata_storage_path", "")
                })
            
            return {
                "success": True,
                "results": results,
                "total_count": len(results)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def process_document_complete(self, document_result):
        """ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        results = {
            "document_id": document_result["document_id"],
            "file_name": document_result["file_name"],
            "processing_results": {}
        }
        
        # 1. AI Search ì¸ë±ì‹±
        print("AI Search ì¸ë±ì‹± ì¤‘...")
        index_result = self.index_document(document_result)
        results["processing_results"]["indexing"] = index_result
        
        # 2. ë¬¸ì„œ ìš”ì•½ ìƒì„±
        print("ë¬¸ì„œ ìš”ì•½ ìƒì„± ì¤‘...")
        summary_result = self.generate_document_summary(document_result)
        results["processing_results"]["summary"] = summary_result
        
        # 3. ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í™”)
        print("ê¸°ìˆ  í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        tech_result = self.extract_technical_info(document_result)
        results["processing_results"]["technical_info"] = tech_result
        
        return results

# ì „ì—­ í”„ë¡œì„¸ì„œ ê°ì²´
document_processor = DocumentProcessor()